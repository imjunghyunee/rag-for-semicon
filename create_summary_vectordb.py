import json
import os
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer
import torch
from pathlib import Path

def main():
    # 1. ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì •
    data_file = "./data/examples_text_summary_pair.json"
    
    if not os.path.exists(data_file):
        raise FileNotFoundError(f"Data file not found: {data_file}")
    
    # 2. JSON ë°ì´í„° ë¡œë“œ
    print(f"Loading data from {data_file}...")
    with open(data_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    print(f"Loaded {len(data)} examples from JSON file")
      # 3. summary í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ Document ê°ì²´ ìƒì„±
    documents = []
    for idx, item in enumerate(data):
        if "summary" not in item:
            print(f"Warning: Item {idx} missing 'summary' field, skipping...")
            continue
        
        summary_text = item["summary"]
        
        # metadata êµ¬ì„± - parent_idëŠ” examples_original.jsonlì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì‹ë³„ìì—¬ì•¼ í•¨
        metadata = {
            "source": "examples_text_summary_pair.json",
            "index": idx,
            "parent_id": str(idx),  # examples_original.jsonlì˜ idì™€ ë§¤ì¹­ë˜ë„ë¡ ì„¤ì •
        }
        
        # id ê°’ì´ ìˆìœ¼ë©´ metadataì— ì¶”ê°€ (Example 1.1, Example 1.2 ë“±)
        if "id" in item:
            example_id = item["id"]
            metadata["example_id"] = example_id
            # parent_idë¥¼ example_id ê¸°ë°˜ìœ¼ë¡œ ì„¤ì • (retrievers.pyì—ì„œ ì´ ê°’ìœ¼ë¡œ ê²€ìƒ‰)
            metadata["parent_id"] = example_id
        
        # original í…ìŠ¤íŠ¸ë„ ìˆìœ¼ë©´ ê¸¸ì´ ì •ë³´ ì¶”ê°€ (ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ì— ìœ ìš©)
        if "original" in item:
            metadata["original_length"] = len(item["original"])
        
        doc = Document(
            page_content=summary_text,
            metadata=metadata
        )
        documents.append(doc)
    
    print(f"Created {len(documents)} Document objects from summaries")
    
    # ë¬¸ì„œ í™•ì¸ (ì²˜ìŒ 3ê°œ ì¶œë ¥)
    for idx, doc in enumerate(documents[:3]):
        print(f"\nDocument {idx + 1}:")
        print(f"Content: {doc.page_content[:100]}...")
        print(f"Metadata: {doc.metadata}")
        print("=" * 50)
    
    # 4. HuggingFace ì„ë² ë”© ëª¨ë¸ ì •ì˜ (ê¸°ì¡´ create_vectordb.pyì™€ ë™ì¼)
    print("Loading embedding model...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="jinaai/jina-embeddings-v3",
        model_kwargs={
            "trust_remote_code": True,
            "device": "cuda" if torch.cuda.is_available() else "cpu",
        },
        encode_kwargs={"normalize_embeddings": True},
    )
    print(f"Embedding model loaded on {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    
    # 5. FAISS vector DB ìƒì„±
    print("Creating FAISS vector database...")
    vectordb = FAISS.from_documents(documents, embedding_model)
    
    # 6. ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± ë° ì €ì¥
    save_path = "./vectordb/summary_faiss"
    os.makedirs(save_path, exist_ok=True)
    
    vectordb.save_local(save_path)
    print(f"Successfully created and saved the summary FAISS vector database at {save_path}")
    
    # 7. í† í° ìˆ˜ ê³„ì‚° ë° ë¶„ì„ (ì„ íƒì )
    print("Analyzing token statistics...")
    try:
        tokenizer = AutoTokenizer.from_pretrained("jinaai/jina-embeddings-v3", trust_remote_code=True)
        
        token_info = []
        token_counts = []
        
        for idx, doc in enumerate(documents):
            tokens = tokenizer.encode(doc.page_content, add_special_tokens=False)
            token_count = len(tokens)
            token_counts.append(token_count)
            
            chunk_data = {
                "index": idx,
                "example_id": doc.metadata.get("example_id", f"Unknown_{idx}"),
                "token_count": token_count,
                "text_length": len(doc.page_content),
                "summary_text": doc.page_content,
                "metadata": doc.metadata,
            }
            token_info.append(chunk_data)
        
        # 8. í†µê³„ ì¶œë ¥
        avg_tokens = sum(token_counts) / len(token_counts) if token_counts else 0
        min_tokens = min(token_counts) if token_counts else 0
        max_tokens = max(token_counts) if token_counts else 0
        
        print(f"\nğŸ“Š Summary Vector Database Statistics:")
        print(f"  - Total documents: {len(documents)}")
        print(f"  - Average tokens per summary: {avg_tokens:.2f}")
        print(f"  - Min tokens: {min_tokens}")
        print(f"  - Max tokens: {max_tokens}")
        
        # 9. JSON íŒŒì¼ë¡œ í† í° ì •ë³´ ì €ì¥
        json_save_path = "./vectordb/summary_token_info.json"
        with open(json_save_path, "w", encoding="utf-8") as f:
            json.dump(token_info, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ Summary token info saved to {json_save_path}")
        
    except Exception as e:
        print(f"Warning: Could not analyze tokens: {e}")
    
    # 10. ê²€ì¦: ë¡œë“œ í…ŒìŠ¤íŠ¸
    print("\nğŸ” Verification: Testing database load...")
    try:
        test_vectordb = FAISS.load_local(
            save_path,
            embeddings=embedding_model,
            allow_dangerous_deserialization=True,
        )
        
        # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        test_query = "calculate crystal structure"
        test_results = test_vectordb.similarity_search(test_query, k=2)
        
        print(f"âœ… Database load test successful!")
        print(f"âœ… Test search for '{test_query}' returned {len(test_results)} results")
        
        if test_results:
            print(f"âœ… First result metadata: {test_results[0].metadata}")
        
    except Exception as e:
        print(f"âŒ Database load test failed: {e}")
    
    print(f"\nğŸ‰ Summary vector database creation completed!")
    print(f"ğŸ“ Database saved at: {save_path}")
    print(f"ğŸ“„ Token analysis saved at: ./vectordb/summary_token_info.json")


if __name__ == "__main__":
    main()
